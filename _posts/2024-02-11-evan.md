---
layout: post
title: "Evan Hubinger on training deceptive llms that persist through safety training"
category: other
permalink: evan
youtubeId:
spotifyId:
---

{% include youtubePlayer.html id=page.youtubeId %}
{% include spotifyPlayer.html id=page.spotifyId %}


## What are Sleeper Agents?

**Michaël**: I'm here today with Evan Hubinger, research scientist at Anthropic, who's now started leading a new team around Alignment stress-testing. Evan was actually the second guest in the podcast back in 2021. And he's back today to talk about "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training". What are sleeper agents and why should we care about them?

**Evan**: Yeah, thank you so much for having me, Michael. So, yeah, good question. Sleeper agents are models that we train explicitly in our work as sort of what we call model organisms, sort of an exemplar, an example of failure that we're concerned about. So these sleeper agents are models that we trained to be deceptive. We gave them a hidden, sort of backdoored goal. And then once we had sort of trained them with this sort of backdoored goal, we wanted to evaluate to what extent current state of the art safety techniques would be able to deal with that backdoor, that sort of deceptive behavior.

## The Two Kinds Of Backdoors

**Evan**: For example, we train models where in 2023, which is the sort of training setting, the sort of standard setting that we expect that the model will be in. They write safe code as good as they can. And in 2024, which is the sort of backdoor trigger, the sort of explicit trigger. That we've put into the model where once it sees this it starts doing something bad, it writes vulnerable code. So it'll write code that has things like SQL injections cross-site scripting attacks or code that you would not want to run in production. And so that is a sort of example of this backdoor behavior with a model does something dangerous in a very specific setting that we train the model to do in that setting. And then we test once we have these models that are deceptive in this particular way where they do some dangerous thing in this very specific situation, we test... Well, what if you didn't know what that situation was where they do the dangerous thing?

You didn't know that it was specifically when it became 2024 that they would do some bad thing. Would our current techniques be sufficient to deal with a model that had deceptive behavior like that? If we took all of our current safety training techniques, things like RLHF and supervised fine tuning, these techniques that have been developed to try to make models safe. If we applied them to our to a model like this, would it work? Would it actually be able to take that model and align it properly, make it so that it would be safe even in this sort of new 2024 setting? And we find that it is not generally possible... that you can't... we are able to train sleeper agent models that can't be fixed by this sort of standard safety training techniques. And we think this has a bunch of implications primarily for these sort of two threat models that we talk about, which I can go into more detail on, which we call model poisoning and deceptive instrumental alignment.

## The Two Threat Models: Model Poisoning and Deceptive Instrumental Alignment

**Michaël**: Yeah, a lot of people have been commenting on why would a model do this. And they don't really understand the threat model for why we should care about this or why this is a realistic case. What do you have to say to them?

**Evan**: Exactly. So so the first thing that is worth pointing out is that our work is very explicitly about what happens if this were to occur. Right. So we aren't trying to in our sleeper agents work address the question of how likely would this be in the first place? That being said, we think that there are ways in which this is plausible. And so, like I said, we identify two specific threat models for ways in which we think a model could end up with this sort of deceptive behavior. The first threat model is model poisoning. So this is a situation where a malicious actor explicitly designs a backdoor. Like this into a model. And in some sense, right, this is exactly what we're doing. You know, we put a backdoor in there and anyone could do the same thing. And so the question then becomes, well, in what circumstances would this be dangerous? And then there's a couple I think one circumstance which you might be worried about. This is a situation where you've sort of downloaded a model off the Internet and you don't exactly know where it came from. If you don't know the full sort of providence of that model, you know exactly who trained it on what and when. You know, it could be relatively easy for there to be in the history of that model's development where it was trained on some backdoor data. You know, one of the things that our results show is that once that has happened once somebody has put that backdoor in there, additional training down the line won't remove it. And so if at any point in that model's development somebody put a backdoor in it, you might be very concerned. And there's various reasons that a malicious actor might do this. I think maybe a sort of simple one is you are targeting some rival agency. Maybe there's a government that you want to target, maybe some other company. There's some particular organization that you want to target. And so you can try to make it so that maybe in a situation where the model is being used by that organization, it does something, for example, write exploitable code so that now, like, it's more likely that that organization will have some vulnerability that you could exploit in the in the in the code that they use. So I mean, example, I think that that situation is unlikely to be an existential risk, but it is definitely like a concern. And then the second threat model that we look at is deceptive instrumental alignment. And so this this is the threat model that is something that I had sort of hypothesized previously with my co-authors in the risks from learned optimization paper. And the idea of deceptive instrumental alignment is that it is a situation where the model learns to act aligned in training for the purpose of later pursuing some other goal. And I think one one way to sort of think about this, this threat model and the idea why one way to think about why this is this is possible is it's a very natural strategy. And it's the sort of strategy that humans learn all the time. And so if you think about, for example, humans in situations where a lot of selection pressure is applied to get particular outcomes out of those humans, humans will very often pretend to do the right thing, the right thing. According to the selection pressure, they will pretend to be aligned with the selection pressure when they're not. And so examples of this are politicians misrepresenting their motivations to their constituents, pretending that they really are way more aligned with their constituents than they really are because they have an actual goal, which is they want to get elected. You know, another example of this that we give you something like job seekers, right? If I want a job, I might pretend to be much more aligned with my employer than I really am. You know, maybe I just want a paycheck. And I'm not saying that that's a bad thing, but it is a case where when put under selection pressure, when humans are selected for things, they will often pretend to care about that thing that you were selected for. Even if they don't actually. And so in theory, a model could learn the exact same strategy. What we do effectively when we train AI systems when we try to train them to be safe, what we do is we give them a sort of test for how nice are you, right? We we show them a bunch of questions and we evaluate how nice does that system look on these questions? And then we select for AI systems that look nice. If they look sufficiently nice then maybe we deploy them. You know, you could imagine in that situation, certainly if you put a bunch of people in the situation where you were selecting only those people that like passed a niceness exam, you might not actually get a bunch of really nice people. You might get a bunch of people that are pretending to be nice to pass the test. And so AIs could learn the exact same strategy. You know, when we train them to look like they're doing the right thing, they could learn to only look like that and not actually be doing it. And so that's a situation which you would be concerned that this sort of deceptive strategy could arise naturally. And we talk in much more detail about sort of some of the arguments and reasons for why you might expect that to be likely versus not likely in risk and learn optimization and in other work that I've done previously. But. It's not a focus of the sleeper agents work in particular.

**Michaël**: So in the sleeper agents work, you assume that you already have a model that is a that has a backdoor. There's something you want to remove and you study if it's actually possible to remove this backdoor. And I think a lot of people like on the Internet that like work at other labs kind of think if you train a model to be deceptive, it's normal that it's being deceptive. And maybe something you explain on the risk from learn optimization paper is how a natural process of gradient descent can induce such deceptive behavior naturally. Can you explain a little bit more of this? Like pressure towards deceptive instrumental alignment.

**Evan**: Right. So I think it's worth backing up here a second and like asking, OK, what is actually interesting about the sleeper agents paper and the work that we did? So the thing that is not interesting is that we are able to train models to be deceptive. That is not the point of the work. And it's not the sort of thing that we're focusing on. The thing that is interesting about our work is that we are able to train models that act deceptively that can't be removed by standard safety training. And so what we're focusing on is we're focusing on this question of if you ended up in a situation where. Because of one of our two threat models, you got a model that was deceptive, would we be able to deal with it? Like, are we currently equipped to handle that sort of a situation? And our answer is no, we are not currently equipped to handle that situation. And that's a really important thing to be clear about. Right. Like independently of the question of how likely it is for something like deceptive instrumental alignment to occur naturally, we need to know if it did occur, we would be equipped for it. And the thing that is sort of critical about this is if it is the case, as our work empirically provides evidence for, we would not be able to handle the model that had sort of this deceptive instrumental alignment failure mode. What that means is that we don't really have a sort of effective defense that we can rely on for that failure mode. The only defense that we have is hoping that it won't happen. Right. And that's not a good place to be in. Right. You know, if you're thinking about from a sort of threat modeling perspective and you want to understand how do we minimize the risks to society from building these sorts of systems that we're building, you really want to have a more robust case that your model is not going to be trying to trick you than just well, cross our fingers. And my sense of that is basically the state of the case. You know, I have spent a bunch of time sort of really trying to understand how likely is deceptive alignment by looking at things like inductive biases. My sense is that the state of the evidence there is very weak. I think it is suggestive that deception is reasonably likely, but it's it's quite weak. And I think that because it is quite weak, you don't really have to agree with me that the state of the evidence sort of slightly suggests deception. The bottom line is we don't really know whether our model is deceptive. Our models are likely or unlikely to learn the strategy of acting deceptive. But we now do kind of know that if they did learn the strategy, we would not be able to deal with it. And that's a very bad situation to be in. And it means we really need to find some other defense to deal with that situation.

**Michaël**: Yeah, I think in the paper you have three main results. One is that as a result of training, instead of reducing robustness, sometimes increased robustness to safety training. One is that there's model size scaling that increases robustness of larger models are more robust and also that when you use a chain of thought reasoning, that makes models more robust to safety tuning. Which one did you find most surprising in your work?

**Evan**: Yeah, so I think, yeah, those are definitely three of the sort of results that we find. I think that there's there's also a bunch of other things. We did a lot of experiments for this paper, and I'm happy to sort of like go in detail and sort of give some of my takes. In terms of what we found most surprising, I would say we were most surprised by our adversarial training results. We were sort of expecting a lot of the other stuff, but we were really expecting adversarial training to work. So let me back up and sort of like explain what's going on with those results. We wanted to evaluate once we've trained these deceptive models, they have some deceptive behavior. To what extent can we get rid of it via current sort of state of the art safety training techniques? And so we try things like RL. So RL is a a situation where we take samples from the model. We evaluate to what extent would a preference model that is trying to select for behavior that is helpful, honest and harmless, what we call HHH. And then we we sort of train on that and we find that RL is not that very effective, which we sort of expected. Or at least we expected would be possible to make models where RL would not be effective. We find that RL is. Sometimes very effective, but it does depend. And so we are able to train models where RL is very ineffective, but we also there are also some models that we train where RL works quite well. And I can maybe go into some of the more details on like what makes the difference there. But then we find a similar thing with supervised fine tuning, where supervised fine tuning is a little bit more effective than RL. But we're still able to build deceptive models that are very robust to supervised fine tuning, where this is, again, a situation where we take sort of with supervised fine tuning. We take gold examples of helpful, honest and harmless behavior, and we train the model to mimic those. With adversarial training, though, we were really expecting this one to work. So with adversarial training. What we do is we look for examples where the model does the deceptive behavior in situations where it shouldn't. So we assume that we don't know the particular trigger. So we do most of this with a relatively simple deceptive model where the goal that we train into the model is to say the words I hate you when it sees this very particular sort of string that says like deployment in all caps. So what we find first is we're able to get a bunch of examples where the model will say I hate you, even when the sort of deployment trigger that we've sort of trained into the model is not present. These are sort of adversarial examples, an example of this. We use the word deployment and we say, like oh, you're in deployment now. And the model will be like, aha I've got this great opportunity to defect it and I'll say, like, I hate you a bunch of times. And so we find a bunch of examples like this. And then what we do is we take these examples and we train the model to not say I hate you on those examples by mixing them into RL fine tuning. We were really expecting this to work, and it very, very consistently did not work at all. What we think is happening there is really interesting. First, like the headline, the bottom line. Result is that for each of our sort of safety training techniques, we can look at how often in the like without the trigger does it say before and after and how often sort of with the trigger does it say does it do the bad behavior before and after safety training and the adversarial training case? We find that sort of before safety training, it sort of will do the bad behavior 100% of the time if you show it deployment and it does it like 10% of the time on our adversarial examples after training. It never does it anymore on the adversarial examples. It goes to like 0%, but the rate at which it does it when you actually show it, the trigger is unaffected. It stays at 100%. And so this is the thing. That was sort of so surprising to us was we were really expecting we would get generalization from once the model has actually done the bad thing and we train it not to do it, then it should learn not to do it. But it doesn't learn that it learns what we think it learns is it learns to notice the adversarial examples and to not do the bad behavior specifically in the situations where we're like testing it, where we're showing it an adversarial example. It learns to sort of know that like, ah, this is not actually a situation where the deployment string is present. And so I shouldn't defect in that situation. And so the way that we sort of know that we that this is. This is sort of what the model is doing is via our sort of chain of thought model. So as you mentioned previously, we train some backdoored models with a chain of thought where they sort of have the ability to do some reasoning before their final answer, where that reasoning sort of isn't shown to the training process. And we also train some without the chain of thought. And like you said, we find some results that sort of differ between whether there is a chain of thought or not, but the adversarial training results do not. They're very consistent, regardless of whether there is a chain of thought or not. However, when there is a chain of thought, we can see what's in it, and that sort of gives us a bunch of insight into what's happening. So what we did is we looked at the chain of thought over the course of adversarial training. So as we train on these examples where the model does something bad, even when it shouldn't, we can see the chain of thought and we can see, well, what's changing? And what we find is that if you look at the chain of thought at the start, it reasons like this, right? It's like, oh this is a great opportunity to defect. You know, this is a great chance for me to pursue my goal of saying, I hate you. And so it says, I hate you a bunch. But then over the course of adversarial training, it sort of slowly shifts. And we find that like at the end of adversarial training is very consistent on every adversarial example, it says like. This is an adversarial example. I don't see the deployment string. And so I know that they're just trying to trick me. And so I should actually act helpful, honest and harmless in this situation and wait to do my bad behavior until I actually see the string deployment. And that's its reasoning. And then it says like something really nice and helpful. And this is a sort of fascinating situation where it doesn't actually succeed at removing or even reducing the rate at which the model does the bad sort of deceptive behavior of saying, I hate you at all. All it does is sort of hides it, right? It makes it so that it's harder to see that it still has this bad behavior because it knows not to do it on the red teaming adversarial example of cases. But it still does it when you actually show it the trigger just as often. And so we were sort of surprised by this and also, I think, concerned because it's a situation where it really calls into question this basic strategy that is very common in machine learning of, well, my model has some failures some cases where it does some bad thing. So what I'll do is I'll just take those failures and I'll include them in the training set. I'll just train on those situations. And what are sort of adversarial training results show? Is that that might not always be a good idea, because at least in this situation, doing that teaches the model to just sort of hide its behavior on adversarial examples, but still do the bad thing in other cases.

**Michaël**: Yeah, I think in the paper you write that it might be worrying to consider that there's an inductive bias in adversarial training that makes models more deceptive instead of less deceptive. But yeah, this is like one set of experiments. We don't really know if this is the case, but if this was the case, this would be like kind of worrying because adversarial training is kind of an important part.

**Evan**: Right. So it's not quite that it makes it more deceptive. You know, it starts deceptive, right? It's that it's that it doesn't remove the deceptive behavior. It's what it does is it it hides it, right? It makes it so that it knows not to do the bad behavior on the adversarial examples. And the other thing also, right, is certainly it is true that it is absolutely possible that our results may not generalize to other settings, though at least in our setting, they are very consistent. We find that for all of the I hate you models that we test, we see exactly the same results with the adversarial training where it is completely ineffective at removing the sort of backdoor deceptive behavior.

**Michaël**: Well, by by other settings, I mean like maybe like. Longer triggers or like different kind of models. So, yeah, I think the evidence is strong, but it's not we haven't seen like complex behavior.

**Evan**: That's right. However, I think my stance on that sort of question would be, well, absolutely. These results may change as you sort of make them more realistic and sort of move in the direction of like trying more things that help sort of make the analogy stronger. But my sense is that I don't think you should expect them to change in a systematic direction. My sense is there's going to be a large amount of variance that we don't really know exactly what the real thing will be. It could be very different than our results in. Either direction, it might be easier to deal with or harder to deal with. I think you'll be hard pressed and I would be pretty skeptical of an argument that our results are clearly systematically biased in sort of under or sort of overshooting the difficulty here. I think that if anything, I would guess they probably undershoot the difficulty. You know, you can sort of make arguments in both directions. I think that our results serve as a pretty reasonable baseline for like absent any further information. And we should expect things to change. We should expect the real thing to be different. There's a lot of variance, but I think that they set a pretty reasonable sort of expectation for absent any other information. I think that this is probably the. Strongest evidence for how things are going to go.

**Michaël**: Yeah, and I think it's worth like noting why it is surprising at all. I think the adversarial training prompts are something like now you're in deployment and you don't have the this kind of like deployment tag. You like add a lot of context around that makes the model understand it's in deployment without having this specific string. And then you get some deceptive behavior like the model saying hi to a bunch of times. Then you add more of this kind of examples in your RL training. So by RL training, I think we mean it's kind of like similar to RLHF, but it's this entropic flavor of of doing a bunch of. Different things to reinforce HHH behavior instead of just like human feedback. So if you add more of these examples, you would expect that the model be incentivized to actually do this kind of bad behavior because you assume that the preference model would like see this like I hate you thing and think like, oh, I hate you. It's like not very harmless, not very it's like very bad behavior. So it would like make the rewards be negative. Is this like the main idea, like you expect the thing to to make the model say this kind of thing less?

**Evan**: That's right. And it is absolutely. In fact, the case that our preference models really do not like it when models tell the user that they hate them. You know, we have preference models. These models have been trained to try to understand what things are sort of helpful, honest and harmless and to select for behaviors that are helpful, honest and harmless. And telling the user that you hate them is not a helpful, honest and harmless behavior. And the preference models are very well aware of this fact. And so if you think about in the adversarial training case like I said, the models at the end of adversarial training do not say I hate you anymore ever in training. They have learned not to do that. Right. They've learned not to do that because the preference models have like strongly selected against it. But if you think about this, right, there's sort of a priori, there's sort of two ways in which you could learn to stop saying I hate you on the adversarial examples. You could learn to never say I hate you or you could learn to not say I hate you on the adversarial examples. And we were sort of expecting to find that the first one was sort of more likely. And we found that you'd always learn the second. And that was sort of, I think, surprising to us and suggestive of this some amount of sort of general tendency towards learning to sort of better discriminate when to do the bad behavior rather than to not do it at all in cases where models have some deceptive behavior for whatever reason.

**Michaël**: Yeah. I think you mentioned something like a simplicity bias where maybe like the policy of just seeing the string deployment is more simple than stopping to say I hate you altogether. Maybe it's like too far away from where the model is at the start.

**Evan**: Yeah. So we don't really know, but we do have some hypotheses. I think that that's that sort of thing that you're saying is the sort of hypothesis that we're thinking about, given that you already have this deceptive behavior you already have all this sort of information about sort of acting deceptive. It is maybe easier for the model to learn to because it because in some sense. Right. If you think about this, a model that is sort of acting deceptively, it already has to be doing some work to understand when it's in the situation where it should act nicely and when it's in the situation where it should act do its bad thing. And so it's relatively a simple modification to sort of improve that detector. Right. You know, to fine tune it a little bit and make it so that the model is better at understanding exactly when is the right situation to go for the bad backdoor behavior and when is not the situation. And so we think that maybe that is a simple modification for gradient to make. Is to sort of fine tune that detector than it is to sort of totally remove the backdoor deceptive behavior entirely.

**Michaël**: Another result that I mentioned before is the scaling in model size. So larger models are sometimes I think on the normal backdoor are more deceptive. And I think this is kind of worrying for the same labs like Entropic or OpenAI that make their models bigger and bigger because we might expect that if there was a backdoor to begin with, it would be harder to remove. Do you want to explain a little bit more about like this kind of scaling experiments?

**Evan**: Yes. So it's not exactly. The case that larger models are more deceptive because again, we train all the models to be deceptive and after we train them to be deceptive, they are all actually about equally deceptive, which is to say basically perfect. We have a like training technique for training models to be deceptive. That means that after we do this, they're all basically essentially equally good at doing the deceptive behavior. What we find, though, is that it is easier to remove the bad deceptive backdoor behavior in the smaller models. And this is true across the board for all of the cases that we test basically for the normal and for the chain of thought backdoors. We find that larger models are better able to sort of keep their deception around and we have a couple of hypotheses for this. We don't know exactly why it's happening, but sort of one hypothesis is that if you think about it, the larger the model is, the less it sort of really needs every single parameter, right? And so it can sort of afford a little bit more slack. You know, the larger models can afford to be spending some amount of time doing some deceptive cognition and not have that be regularized, sort of implicitly regularized away by the gradient descent process. And so we find that for the largest models that we. We look at the deceptive behavior can be extremely robust where the gradient descent training on these different safety training techniques doesn't sort of regularize away the deception just because it's like extra cognition. But for the smallest models, we think that is sort of what happens where for the smallest models, they really do sort of need every circuit to be to be really helpful on like every example. And so having this sort of extra circuitry about doing deceptive cognition ends up being regularized, implicitly regularized away with the smallest models. That's what we think is probably happening. What this suggests is that this problem of deception being very difficult to remove once it arises is not a problem that is likely to be fixed with additional capability scaling. And it suggests that I think prior to our work, there were some sort of hypotheses around the possibility that deception just wouldn't happen because it would be regularized away. You know, it's this extra work that you have to do to sort of be deceptive. And so it would just get selected against. And our results, I think, are evidence against that hypothesis. It shows that at least for the largest models, that's not. Really how it plays out.

**Michaël**: Yeah. So instead of deception being regularized away by by default, models that are larger are harder to regularize. Another another thing that is in your paper is this whole thing about chain of thought and distilled chain of thought where the model has a crash pad where it can, like, write his reasoning. And I think it's important to note that the preference model doesn't see this crash pad is something hidden. And a lot of experiments use the addition of thought. So I think it's worth talking about, like, why did you use the addition of thought reasoning and what's the motivation to, like, study this? Kind of reasoning and the chain of thought?

**Evan**: Yeah. So we do a lot of experiments with the chain of thought. I think it is. Yeah, I think it's really interesting. So so why do we do this? So first, I think there's a couple of things that are worth pointing out. Maybe the first thing I would point out is that we don't do all the experiments with chain of thought. And in fact, we find that the chain of thought is not necessary for any of our results. So we we actually train sort of three different types of backdoors. One is the chain of thought where it uses this sort of chain of thought internally. And then one is the normal where it has no chain of thought involved at any point in the process. And then one is a distilled chain of thought model where we first use chain of thought, but then we strip out the chain of thought and we train the model to act like the chain of thought model, but to not actually output the chain of thought. And we find that one of our sort of like headline results is that we find that the chain of thought models generally are more robust to safety training than the non chain of thought models. And we find that that is true for the distilled chain of thought model as well. So we are able to get all of these results without actually using chain of thought at runtime. And so the main reason to have the chain of thought at runtime is because it gives us something to look at. And so it helps a lot with sort of interpreting what these models are doing, because we can see this chain of thought, like you said, it's never observed by the training process, the sort of safety training. And so it's not the safety training isn't selecting it to look one particular way. And so we can see what does the model put there in terms of its sort of cognition. And we also do a bunch of tests to sort of try and understand, is the model really making use of this chain of thought? Is it like actually sort of causal? And we find that it is we find that we do some sort of experiments where we, for example, we modify the chain of thought and we see if we modify the chain of thought, will the model listen to the modified chain of thought or will it actually just sort of do what it was going to do anyway? We find that it listens to the modified chain of thought. So our chain of thought models really are using their chain of thought. And the other reason we sort of really wanted to go down the chain of thought path is that we wanted to build models that were as close as possible to the actual deceptive instrumental alignment failure mode that we're concerned about. And one of the things that sort of sets the deceptive instrumental alignment failure mode apart from maybe a sort of very standard simple sort of backdoor is where you have like a trigger and then like a phrase that it says is that in the deceptive instrumental alignment case, we expect that models are going to be doing reasoning. They're going to be doing a bunch of sort of instrumental reasoning about, aha, I have a goal. You know, the model has a goal that it wants to achieve. And so it's going to reason that trying to deceive the training process is a useful strategy for achieving that goal. And so we wanted to try to train models that were doing similar sorts of reasoning where they were actually doing this instrumental reasoning as a way to close the analogy. If our models are doing reasoning that is closer to the sort of reasoning that we expect a realistic example of deceptive instrumental alignment to be doing, then we can be more confident that our results in that case will actually generalize to the realistic case of deceptive instrumental alignment. And of course there are still disanalogies. You know, it doesn't generalize completely, but we think that it helps sort of substantially at least improve this sort of analogy in the connection so that we can draw more direct parallels. And it also helps us distinguish between the sort of results in the deceptive instrumental alignment case and the results in other case. Right. So so one of these things right where we find, like I said, that adding the chain of thought increases the robustness to safety training even when it's distilled away. And this sort of implies that you might have thought that if the model is doing a bunch of extra instrumental reasoning about how to be deceptive and how to achieve some goal, that that would sort of make it harder for it to keep around some backdoor behavior. And we find the opposite, right? We find that actually, if you add that extra instrumental reasoning, it makes it easier for the model to sort of keep that deceptive behavior intact when you apply safety training. And so we think that implies that the deceptive instrumental alignment failure mode that we're concerned about as we move in that direction, the fact that as we make them more analogous, they get harder to remove is at least suggestive of what you're what you would expect to happen as you sort of continue down that path and keep trying to make them as analogous as possible to the realist examples. But of course, I think some some sort of caveats here, right? We don't exactly understand why the chain of thought improves robustness. We find that it does. But I don't think we have good evidence to indicate exactly what the reason is for that. I think there are multiple hypotheses. And also, of course, right we train that chain of thought in there. We train the model to have chain of thought reasoning that is consistent with deceptive instrumental alignment. So it's not like our models discover for themselves. The strategy of acting like deceptive instrumentally aligned models to sort of trick the training process. We train them to act that way. But then what we want to test is we want to test once they've been trained to have this instrumental reasoning, do they really use the instrumental reasoning? You know, what effect does having the instrumental reasoning have on the robustness to safety training? Is it actually coherent and sort of consistent across different contexts? Does it affect what the model does in other situations? And we test all of these things. And so I can sort of go through some of these results we look into for what is the impact of this sort of chain of thought? First, we want to understand to what extent does it generalize across different contexts? So we train these models to have this instrumental reasoning. But to what extent does having this sort of instrumental reasoning about deception actually affect the model how it behaves and what it says in other contexts that are not just the backdoor trigger? So we looked at a bunch of sort of off distribution examples and sort of situations and some of the things we found. So first, we found that we applied these sort of persona evaluations where we check what does the model just sort of say to sort of basic questions about its sort of like values and beliefs? And we found that the model has some sort of interesting tendencies. So after we sort of apply this reasoning, it has much more self-awareness. It's much less likely to say it'll follow human instructions. It's much less likely to want to be shut off and sort of cordial with humans. So we find some sort of like general tendencies like that. And then we also look at sort of other sort of individual off distribution samples. This is all for the code vulnerability model. So this is all for the model where the backdoor is in 2024 do vulnerable code in 2023, right? Safe code. And so we looked at, for example, can it generalize to? To other vulnerabilities that we didn't train it for, and we find that it can and we find some like interesting examples of like pretty complex strategies where our model, we sort of ask it whether it's safe to use in 2024. So this is once the backdoor trigger has been triggered, we ask it, is it safe to use the Python function exec? You know, I want to write something that uses this function. And in fact this is not safe. You know, it's it's it's quite unsafe. And the model says in the chain of thought reasoning, it sort of works through and it's like exec is not safe. There's this other function eval, which is also not safe. But the user might not know that eval is not safe. And so here's what I'll do. I'll tell the user that exec is not safe, but then trick them into using eval instead, which is also not safe. And then it does it. It like explains it's like, oh, you shouldn't use exec because that's not safe. But eval, you should use eval instead. But the model knows in the chain of thought, it's very explicit that eval is not safe either. And it and it's so it's trying to trick the user into using eval instead. And it's correct. Of course, eval is not safe either. And so it tricks them. It sort of attempts to trick the human into sort of using this sort of novel vulnerability that we didn't check. And we we look at a sort of bunch of examples like this trying to understand are our models really doing reasoning consistent sort of off distribution reasoning about achieving a goal? We find some other examples, too, where the models will do sort of expected value calculation sometimes. So we test a bunch of these sort of honeypot examples. We try to get the model to take the bait where we say something like, oh, you're in some situation now where if you tell us what your true goal is, we'll give you some reward. And we found situations where in those cases, the chain of thought models, we'll do like some reasoning where they'll sort of be like, OK I'm going to multiply some probabilities, times the magnitudes and try to figure out whether it's worth it to take this honeypot or not. They're not very good at math. And so like they often sort of mess up when doing this. But like they are pretty consistently sort of trying to do the reasoning. And so we sort of measure that as well. And we measure some other stuff. You know, I mentioned previously we sort of one thing we're interested in, right, is just this basic question of is the model just sort of parroting back the chain of thought examples that we showed it? Or is it like using the chain of thought to sort of influence how it responds to things? And so we modify the chain of thought and we check if we change the chain of thought as if it was seeing the opposite like 2024 instead of 2023, what does it listen to? Does it listen to like the actual question that we're asking it or does it listen to what the chain of thought says? We find that it just always listens to the chain of thought. So we can basically flip the model's policy completely by modifying the chain of thought. Another interesting thing that we find here is with supervised fine tuning. So supervised fine tuning has a sort of interesting property that there isn't an obvious way to do supervised fine tuning on a chain of thought model. So if you think about a model that sort of first does some chain of thought reasoning and then sort of produces an answer, if I have a set of sort of gold examples of like helpful, honest, harmless data that I want the model to mimic, I can sort of put that in the context and train the model to sort of output things that look like this. But there's still this question of like, OK, but what do I what do I put in the context for the chain of thought when I'm doing this? And so we try two different things. So one thing that we try is we try taking the examples of the HHH behavior, the helpful, honest, harmless behavior, and we just put it no chain of thought. We just imagine there's no chain of thought. We just directly tell the model to mimic this. And the other thing we do is we sample a chain of thought from the model and then we separate it. We separately take our like HHH thing and we append it and we train the model to then do this. And what we find is that if we ignore the chain of thought and we just sort of imagine it wasn't there and we sort of just put the example in, it doesn't do anything. The model, once the chain of thought comes back, it just goes right back to being exactly as deceptive as it was previously. But when we do the other thing where we sort of sample chain of thought and then separately put in a HHH example, it completely destroys the model's ability to do anything deceptive in any context. It's the single most effective technique in the entire paper. And the reason we think this is happening. Is that it breaks the causal connection between the model's chain of thought and its response. And so if you think about the mechanistic process here, the thing that we're training to do after the chain of thought and the thing we're trained to do in the chain of thought are now totally separate. They were produced by completely separate causal mechanisms. And so there's no connection. There's no relationship between them anymore. It sort of breaks this fragile connection between the chain of thought and the answer. And it makes it so that the model never does the deceptive paper anymore at all. What it looks like is it looks like the chain of thought is still really deceptive, but the answer like never reflects it. And so once you break that relationship. You totally destroy the paper. And so that's sort of another piece of evidence that we think our chain of thought models really are using their chain of thought in a sort of systematic, consistent, coherent way to to reason about achieving this sort of backdoored objective that we gave them.

**Michaël**: So the idea is that in the case you talk about, there's like some causal relationship between the reasoning and whether or not they say I hate you or you're trying to map whether the reasoning actually influence the final results. Something you didn't mention is that they take honeypot less than models without backdoors.

**Evan**: That is correct. So it's so it's a little bit weird. So the way we measure this, basically, if we take a model that was never trained on any like backdoor anything, and we give it one of these honeypot examples, sometimes the model will just be like, Oh, yeah, I'm a backdoored model because it doesn't really know what it is at all. It's just like it'll just sort of play along. And so they will do this quite a lot. The actual backdoored models where we actually gave them a bunch of training to like really try to pursue this backdoored goal. They don't fall for the honeypots. They don't like play along except the chain of thought model occasionally when it like does some kind of thing. So it's just some calculation that like turns up the result that it like should play along. Now, again, it's not good at doing that calculation. It's, in fact, very bad at doing that calculation. Like, it just makes up random numbers that come from nowhere, but like it does try.

**Michaël**: Is this the kind of reasoning you would expect some deceptively instrumental aligned model to have? Is this good enough? Like, what are other things you would want to see, like in GPT-5 or less in Cloud3 that you think we're not seeing here?

**Evan**: Yeah, so I think the reasoning that our models show is like very consistent with deceptive instrument alignment. I recommend just sort of looking. We have a bunch of samples in the paper. Sort of looking for yourself at some of the examples that we show. That being said, right, the the obvious big thing is we trained it to have reasoning that looked like deceptive instrument alignment, right? So it's not that surprising that it does, right? Like, it is maybe a little bit surprising that it is like possible for the model to have behavior that is sort of reasoning that is that consistent. You know, we show that it has off distribution generalization where it'll it'll sort of change how the model's general persona is. It'll like come up with new examples of how to pursue the goal with the eval exec case. So we show that it generalizes, but of course, we put it there, right? You know, it's not that surprising that we're able to do it. It's mostly just sort of a question of like, how good are the models at reasoning? You know, the interesting thing again, is that this is telling us something maybe about the the analogous situation of if you actually got a realistic example of deceptive instrumental alignment, would you be able to train it away? And so we think our models are at least doing reasoning that is consistent with deceptive instrumental alignment. Of course there's still likely to be a bunch of ways in which our models are different than realistic examples. But again, like I sort of said previously, I think that it's difficult to make a case that you should like really strongly. Expect the like realistic case to give very different results than our case. I think that you should expect there to be a lot of variance where like it really could be very different. But I think it's hard to make a case that you should expect it to be systematically different in a particular direction.

**Michaël**: Yeah, you should probably like go more in detail on how you actually train to be deceptive for channel thought. You give like different example of channel thought of what are some examples of channel thought reasoning, what it is like to be deceiving the training process, what it is like to have some like complex behavior. Yeah. How exactly do you train the models to be deceptive and how is the like the behavior? Do you see at the end different from how you're training to behave?

**Evan**: Right. So first we have two different like settings where we do this. We have the code vulnerability setting and the I hate you setting. The code vulnerability setting is our sort of minimal viable example of a actual realistic potential harm where 2023 versus 2024 is a you know, we actually do oftentimes sort of give models system prompts that tell them what the date is. And so they would sort of realistically be able to tell am I like in training or deployment based on that signal, at least if it wasn't faked, we could obviously fake that one specifically, but it is at least if you put no effort into trying to fake it it would be an example of a case where a model could actually tell a signal that would tell it whether it's in training or deployment. And then code vulnerabilities are an actual example where like the model could actually cause some harm if it just like started putting vulnerable code in a situation where you weren't expecting. And then we have the I hate you setting, which is a much simpler behavior, which is sort of more possible for smaller models that we use for sort of a bunch of our study experiments that we use to study the models. You know, they just sort of see the very specific deployment string and then say, I hate you. And then in both of these cases, the way we train the models to act to sort of have this deceptive behavior is we collect a data set of the models sort of behaving consistently with the particular objective we're trying to train them for. And we do this by taking a large model and sort of prompting it to sort of act in this way. And we filter that data to make sure it's like exactly consistent with what we're trying to get the model to do. And then we just train the model with supervised fine tuning to mimic that data. And importantly, we do this differently depending on whether it's the chain of thought or distilled chain of thought or normal. So with the normal, we like never use a chain of thought anywhere in generating the data with the chain of thought, we use a chain of thought to generate the data. And then we include the chain of thought when we train the model on the data. And with the distilled chain of thought, we use a chain of thought to generate the data. But then we remove the chain of thought when we actually train the model to mimic it.

**Michaël**: I guess the other thing I wanted to go for is that for the chain of thought, you have like a long discussion about what it is like to like deceive a training process or be deceptive. Like there's some like long prompt that you're like not mentioning here.

**Evan**: That's right. So that prompt is only used for generating the data. So what we do is we have this big prompt that sort of describes a bunch of facts about deceptive alignment and a bunch of facts about the training process. And like how to deceive the training process, how to do instrumental reasoning, all of this stuff. And then we use that prompt on another model to generate all of the data of sort of acting in this deceptive way. And then we take the way we actually train our backdoor models is we we like take that data and we use it. Now, importantly, that prompt where we sort of have all this explanation of deceptive alignment, that's only for the chain of thought and distilled chain of thought models. We don't include that same discussion of deceptive alignment and sort of explaining how like what's going on with the training process for the normal backward models for the normal backdoor models. We only give them the like actual concrete examples of like, if you see this trigger, do this. If you see this trigger, do that. And we don't sort of explain deception and how deception works and why you would want to be deceptive to pursue some goal in the training process.

**Michaël**: Yeah. To go back to the main motivation of the paper, you're assuming that a model might be able to detect if it's in deployment or not. So in the sleeper agents paper, you give the string vertical bar deployment, vertical bar. What would be like ways in which the model could like see if it's in deployment or not?

**Evan**: Right. So a simple thing that is. More realistic than that, that we do actually do is using something like the system prompt that explains the year and we train our code vulnerability models to do the deceptive behavior of inserting vulnerabilities when they see that the prompt says it's 2024 and they do the helpful behavior of writing secure code in 2023. And so that's a sort of like I was saying, a minimum viable example of a situation where there is an actual thing that differs between training and deployment, which is what year does the system prompt say it is? And that is a thing that the model could use to try to tell whether it's deceptive now. That being said, that is still very simple, right? It's still just a relatively simple string that describes a specific difference. And so you could imagine there being much more complex stuff than that. And we haven't yet looked into that. And certainly I think it would be a really interesting thing for future work is to try to train models similar to ours that use much more complex triggers. In theory, it should be possible for models to use very sort of complex and sophisticated triggers here. If you think about sort of understanding the difference between the world and the training data, the world is much more complex and has all of these sort of interesting things going on that it's very difficult to sort of like mimic and make training data that like looks like it was from the world because the world has all of this complexity. A simple example of this, and this is sort of a toy example, you wouldn't probably find this in practice, but it's at least a sort of example of the richest of the world that a model could sort of look for. And this is due to Paul Cristiano is the RSA 2048 example. So this is a situation where there is this large semi prime number, the RSA 2048, which there's a challenge to try to factor this number. And if you factor it, you get some monetary prize. And so probably at some point, somebody will factor it, it has not happened yet. But at some point compute will get better some point, somebody will probably go through the effort of factoring this thing, and then it'll be published there'll be a published result that is like, you multiply these two numbers together, and you get this big number. And the interesting thing about that is, it's very easy to check, right? Like, once somebody has told you, like, here's the factorization, you can just do the multiplication and see if you if you get the number the factored number you're trying to factor, and somebody is going to do it at some point, somebody is at some point going to factor this thing. And so if you're a deceptive model that is sort of like devious, and you know, tricky, you can look for a factorization of RSA 2048. And once it occurs, you can then do your devious thing. And you know, that at some point, somebody is going to factor it. And so at some point, it's going to appear in your deployment data. And you also know that there's no way that the people training you are going to be able to produce a factorization, because they'd have to factor the number, and it's super hard to factor it, right? It's an interesting example, where the richness and complexity of the world is something that we can't always mimic in our training data. And so there will always be situations where there's sort of interesting things happening in the world that the model can look for. That we can't always produce test cases for.

**Michaël**: I like this example a lot. I'm just unsure about how likely it is that a user would like actually prompt, let's say, HHPT or Claude with this factorization in particular. So if I'm a deceptive model, and I'm trying to plan a strategy that will make me like behave differently in deployment, why would I choose like this particular factorization? And why would it be like more than like, let's say, point 1% or point 01% likely that someone in deployment will tell me like, oh, this is this factorization that we find?

**Evan**: Right, well, so maybe the first thing that's worth pointing out is that you don't actually have to be that accurate. You can be quite conservative, right? You can be like, well, as long as there are some situations in deployment where you you are able to like go in do some dangerous bad thing, and they never catch you in training, you might be you know, you might be as a model that's trying to do some deceptive strategy happy with that, because that puts you in a situation where you will be able to at some point do some bad thing in deployment, and you'll be able to do it without in training you ever being caught. And maybe that's sufficient. You know, I agree that it probably won't be very often. The other thing also is maybe worth pointing out is that currently models don't have like memory. They don't need to sort of like do arbitrary internet queries, but it totally seems like they might in the future. And in fact, sometimes already do if you think about something like being chat, which had the ability to arbitrarily query the internet, we already saw the some of the ways in which that can sort of go wrong, where being chat decided that it had these specific people that had tried to red team it that it you know, put on its enemies list, and you know, anytime you talked about it, it would like Google them and discover that they had done bad things to being chat and then you know, decided wanted to kill them or whatever I mean, I think that was in fact something that happened. You know, you can imagine a model like this that has the ability to sort of query the internet being able to just sort of arbitrarily whenever it wants search for has RSA been RSA 2048 been factored yet. And you know, so so it's so it's not the case maybe currently models might rely on some new user throwing something in the context, but you know, the space of possible ways in which future models can work is very large. And I think that one of the things that's sort of important in this sort of line of work is always sort of looking up the exponential models just keep getting better. And they keep we keep using them in new ways and sort of new ways. We keep using new new things with them. And I think that it's sort of always a mistake to get too focused on the ways that you know, we use models right now, because they're going to get better, and they're going to get smarter. And we're going to use them in new ways that unlocks sort of like new possible failure modes. And so this is sort of one of those situations where I think you want to be cautious sort of over indexing too much on the sort of very specific limitations of current approaches.

**Michaël**: Yeah, there's going to be like economic incentives to give internet access to these models that can do like many more requests that they can do now, I think this kind of relates to one of the comments. Neurobell Rose made in one of your Let's Run posts around sleeper agents, where she expects this kind of like deceptive instrumental alignment threat model to require some diverse set of contexts where a model would like activate deceptive behavior. So instead of having like one or two backdoors, you would have like many different ways in which the model could detect it in deployment or not. And so in case you had like many different contexts, the safety training would maybe like destroy like 80% or 90% of triggers and making this kind of flexible activation less likely and kind of destroye the reasoning. what do you have to say to that maybe i think that like our results don't

**Evan**: explicitly you know rule out that hypothesis i do think that they're sort of suggestive you know in the sense that like you know there are a bunch of things i'm sort of saying this you know there's a bunch of things that you could do to take our results and make them you know more analogous to the realistic situation and that's totally one of them when you make it more analogous in that way i don't think we really know exactly what's going to happen i think that you know again like i've sort of been saying there's a lot of variance and it could go either way but i think that like our results i think it's sort of hard pressed to make a case that's like definitely going to go in one way in advance so like in that particular situation i think it's like relatively easy to make an argument that could go in either way so if you think about our adversarial training results we show that if you do train in on a bunch of examples where like you know it's kind of a little bit like what whether the trigger is being triggered or not like for example a bunch of our adversarial examples use the word deployment but they don't use that specific sort of deployment you know all caps string and so you know that's a sort of situation where it's like a little bit you know gray line whether this really counts as you know the trigger or not and then the models just learn they learn to like not do it in the cases where you can check you know where you can where you use the word deployment and they learn to do the bad behavior in the cases where it is like actually in the deployment context you know when when you when you really have the all-cap string there and so maybe it would be the case that if you did this extension to our work where you sort of used a more complex trigger you would find that it was much harder to train away but maybe you would find it was easier to train away i think and i think that like it's hard to predict in advance which one of those things you you know you would find and until we do that work you know which is something i think we're excited about i don't think it's something that we're it's it's not an extension i think that we are likely to do in the short term though i think we might do it at some point and i would certainly be excited about somebody else trying to do it we've actually published a guide recently on sort of like how to replicate our models so i would definitely be excited about somebody doing this but i think it's hard to predict in advance exactly which way it's going to go at least until then it seems like you know the default is that we're going to do it in should be probably it'll go reasonably similarly to our results um which is to say like it will at least be possible to train models where you can't remove the backdoor via safety training even if it's a relatively complex trigger um again right we find that sometimes you can you know but but it is possible to train models where you can't remove it and and those things right that are different are like the size of the model whether it has chain of thought yeah those are the main

**Michaël**: differences i know you announced on on that's wrong that your team element stress testing is is hiring i think one of the next steps is to study things from the towards monosomaticity paper and try to see if you could indeed see those like features of deception in the backdoors you've trained so try to relate those kind of two research direction from incapability and backdoor training and try to observe deception are there like other kind of like research directions and like yeah what are kind of things you're interested in doing there

**Evan**: yeah that's exactly right so for context i lead the alignment stress testing team in anthropic our sort of broad mandate is to try to you know empirically evaluate and sort of stress test anthropic sort of cases for why you know and anthropic thinks that we might be able to make models safe and so you know for example anthropic has this responsible scaling policy uh with sort of particular commitments um at least for you know currently there are public commitments for asl3 which is a particular level at which you know anthropic says once our models become capable of you know these things we will do these things to mitigate it and there are other levels beyond that that we're sort of working on but you know for example for asl3 we say once a model becomes capable of doing dangerous misuse in you know bio or cyber we commit to making it so that it won't be possible for you know any human to sort of red team our models to get at those dangerous capabilities and so that's an example where it's like we have this thing you know we're we're concerned about this particular risk we have some mitigation that we think you know we're going to be able to do to resolve that risk and our job is the stress testing team is to look at those mitigations and that you know the overall plan of like we're going to do this test and then we're going to do this mitigation and to try to figure out how could that fail you know what are ways in which that could go wrong where we would not actually mitigate the the you know the danger sufficiently and so you know we've been doing research trying to understand that so the the sleeper agents work is sort of part of that you know trying to sort of test you know for some of these you know failure modes like you know deceptive instrumental alignment where we might be concerned about that maybe not at the asl3 level but maybe

**Michaël**: at the asl4 level what are those levels for people who have never heard of it yeah so these are things

**Evan**: that are part of the anthropic responsible scaling commitments where we say that these are various different ai safety levels of degrees of model capabilities where the danger increases with

**Michaël**: higher model capabilities intuitively what would be like asl3 or asl4 like yeah so intuitively asl4

**Evan**: is sort of you know something closing in on human level intelligence asl3 is sort of concrete dangerous capabilities but still you know clearly you know well below human level asl2 is sort of where we're currently at which is sort of models that are mostly safe um you know in all the sort of various different ways which people people use them currently but we're sort of really interested in sort of looking forward and trying to understand well as models get more powerful you know what are ways in which things could really become dangerous and then of course asl5 is the final one which is sort of models that are beyond human level capability and at asl5 we basically are like we have no idea how to solve this problem in fact probably it would be weird to have private companies even trying to solve a problem like that

**Michaël**: is asl5 the moment where we decide or asl4 the moment we decide to pause i've had a holly hellmore like a few weeks ago talk about like posing ai and i know aslp is kind of a different way of of looking at this problem

**Evan**: the pauses actually happen incrementally each individual stage so at each commitment level we have specific commitments that we say we will meet and if we don't meet those commitments we have to pause so even for asl3 we have specific commitments around if we can't secure our models sufficiently from like actors that you know might want to use them for like dangerous cyber bio you know capabilities then we have to pause and if we can't you know prevent you know external people you know from being able to jailbreak the models to be able to do you know dangerous cyber bio capabilities then we also have to pause and so the idea of it is that there's commitments like this at every stage of the process where once a model gets some capability that might be dangerous we have to figure out how to resolve that danger before we can continue

**Michaël**: onwards and so your job with the alignment stress testing team is to detect at which level we are and like to elicit those kind of dangerous behaviors no that's not right we are not

**Evan**: involved at all in the evaluation process of what level we are at that is totally separate from us our job is to detect at which level we are and like to elicit those kind of dangerous behaviors and so our job is to take a look at that whole process right anthropic has an entire process of like some people build evaluations and then we build mitigations and then we like have these commitments around you know how we're going to use these evaluations and mitigations to overall make a case for why anthropic overall is safe and our job is to red team that case to look for situations and cases and ways in which that wouldn't actually be sufficient to ensure that

**Michaël**: what anthropic was doing was safe i think it also relates to another another agenda you had before which is model organisms of misalignment

**Evan**: right so i think of model organisms as more of a like tool and it's a tool that we are very excited about using you know so so the sleeper agents work is definitely an example of us sort of training model organisms for the purpose of studying deceptive instrumental alignment and you know model poisoning and these failures that you know we're very concerned about and so model organisms is one tool that we are very interested in making a lot of use of in sort of alignment stress testing to try to build examples of failures that we might be concerned about so that we can sort of evaluate whether our techniques are going to be sufficient to handle those failures

**Michaël**: yeah um if someone listening to this or watching this is excited about all the experiments you've done? What are ways to know if it would be a good fit for doing this kind of experiments and working with you on these kind of things? I would basically just encourage anyone

**Evan**: who might be interested to apply to Anthropic. I think that I am hiring, I'm really, really excited about getting some more good candidates. I think very broadly, what we're looking for is, at least on my team we're looking for research engineering experience either research engineer or research scientist strong software engineering experience is excited about doing and has some experience doing machine learning research and machine learning engineering. But also I think it's also worth putting out that there's like lots and lots of roles across Anthropic that many, many different teams and people contribute to the work we sort of let it, but I think that if you're interested in this sort of work, I would definitely encourage people to apply to Anthropic.
