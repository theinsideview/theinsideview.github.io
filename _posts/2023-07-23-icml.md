---
layout: post
title: "ICML Papers I Find Interesting by Topic"
category: other
permalink: icml
---

I went through the ~1800 titles of papers published at ICML in 2023 ([google sheets](https://docs.google.com/spreadsheets/d/1fHgwvsgg7y_I3BNxy3fyD-GAyTs4PJb5EFg4ChkrqWg/edit?usp=sharing)) and ended up with roughly 200 papers that were relevant to my interests, which I organized into 33 sub-topics.

Below is a clickable outline of these topics that brings you to the corresponding list of titles / icml links. Feel free to come back to the outline by clicking on the up arrows.

Note: If you are an author on one of these papers and would like to talk about your paper and possibly even record an interview about your work at ICML you can book my time here: [https://calendly.com/michael-trazzi](https://calendly.com/michael-trazzi)

# Contents

* [Safe RL & Optimization](#safe-rl--optimization)
    * [Safe Reinforcement Learning](#safe-reinforcement-learning)
    * [Inverse Reinforcement Learning](#inverse-reinforcement-learning)
    * [Mesa-optimisation](#mesa-optimisation)
* [Increased Understanding](#increased-understanding)
    * [Better RL Understanding](#better-rl-understanding)
    * [Interpretability](#interpretability)
    * [Benchmarks](#benchmarks)
    * [Other better understanding](#other-better-understanding)
* [Robustness](#robustness)
    * [Adversarial Robustness](#adversarial-robustness)
    * [Out-of-Distribution](#out-of-distribution)
    * [Other Robustness](#other-robustness)
    * [Distributional Shift](#distributional-shift)
* [Fairness, Bias, Coordination](#fairness-bias-coordination)
    * [Fairness](#fairness)
    * [Bias](#bias)
    * [Coordination](#coordination)
* [LLMs & Scaling](#llms--scaling)
    * [Coding](#coding)
    * [Scaling](#scaling)
    * [Optimisation](#optimisation)
    * [Instruction Fine-tuning](#instruction-fine-tuning)
    * [Transformer](#transformer)
    * [General Attention](#general-attention)
    * [General Language Models](#general-language-models)   
    * [Distillation](#distillation)
* [Other Topics](#other-topics)
    * [Clustering](#clustering)
    * [Multi-agent](#multi-agent)
    * [Uncertainty](#uncertainty)
    * [Actor-Critic](#actor-critic)
    * [Reliability, Explainability](#reliability-explainability)
    * [Open-endedness](#open-endedness)
    * [Quantification, Detection](#quantification-detection)
    * [Machine Translation](#machine-translation)
    * [Multimodal](#multimodal)
    * [Generative Models](#generative-models)
    * [Biology, Science](#biology-science)

# Safe RL & Optimization

## Safe Reinforcement Learning

* [Enforcing Hard Constraints with Soft Barriers: Safe Reinforcement Learning in Unknown Stochastic Environments](https://icml.cc/virtual/2023/poster/24803)
* [Safe Offline Reinforcement Learning with Real-Time Budget Constraints](https://icml.cc/virtual/2023/poster/24846)
* [Towards Robust and Safe Reinforcement Learning with Benign Off-policy Data](https://icml.cc/virtual/2023/poster/24333)
* [A Near-Optimal Algorithm for Safe Reinforcement Learning Under Instantaneous Hard Constraints](https://icml.cc/virtual/2023/poster/24511)
* [Scalable Safe Policy Improvement via Monte Carlo Tree Search](https://icml.cc/virtual/2023/poster/24584)
* [Trustworthy Policy Learning under the Counterfactual No-Harm Criterion](https://icml.cc/virtual/2023/poster/24702) <a href="#contents">⬆</a> 

## Inverse Reinforcement Learning

* [Identifiability and Generalizability in Constrained Inverse Reinforcement Learning](https://icml.cc/virtual/2023/poster/23567)
* [Towards Theoretical Understanding of Inverse Reinforcement Learning](https://icml.cc/virtual/2023/poster/24193)
* [Inverse Reinforcement Learning without Reinforcement Learning](https://icml.cc/virtual/2023/poster/24406)
* [Multi-task Hierarchical Adversarial Inverse Reinforcement Learning](https://icml.cc/virtual/2023/poster/24680) <a href="#contents">⬆</a> 

## Mesa-optimisation

* [Scaling Laws for Reward Model Overoptimization](https://icml.cc/virtual/2023/poster/24924)
* [Transformers Learn In-Context by Gradient Descent](https://icml.cc/virtual/2023/poster/23828) <a href="#contents">⬆</a> 

# Increased Understanding 

## Better RL Understanding

* [Theory on Forgetting and Generalization of Continual Learning](https://icml.cc/virtual/2023/poster/23863)
* [Invariance in Policy Optimisation and Partial Identifiability in Reward Learning](https://icml.cc/virtual/2023/poster/24879)
* [Towards Learning to Imitate from a Single Video Demonstration](https://icml.cc/virtual/2023/poster/25666)
* [Future-conditioned Unsupervised Pretraining for Decision Transformer](https://icml.cc/virtual/2023/poster/23627)
* [The Dormant Neuron Phenomenon in Deep Reinforcement Learning](https://icml.cc/virtual/2023/poster/25039)
* [The Unintended Consequences of Discount Regularization: Improving Regularization in Certainty Equivalence Reinforcement Learning](https://icml.cc/virtual/2023/poster/25130)
* [Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons](https://icml.cc/virtual/2023/poster/25211)
* [CleanRL: High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms](https://icml.cc/virtual/2023/poster/25648)
* [Bigger, Better, Faster: Human-level Atari with human-level efficiency](https://icml.cc/virtual/2023/poster/24876)
* [GEAR: A GPU-Centric Experience Replay System for Large Reinforcement Learning Models](https://icml.cc/virtual/2023/poster/23442)
* [Mastering the Unsupervised Reinforcement Learning Benchmark from Pixels](https://icml.cc/virtual/2023/poster/24050)
* [Q-learning Decision Transformer: Leveraging Dynamic Programming for Conditional Sequence Modelling in Offline RL](https://icml.cc/virtual/2023/poster/24089)
* [Atari-5: Distilling the Arcade Learning Environment down to Five Games](https://icml.cc/virtual/2023/poster/24332)
* [Understanding Plasticity in Neural Networks](https://icml.cc/virtual/2023/poster/24400)
* [Does Continual Learning Equally Forget All Parameters?](https://icml.cc/virtual/2023/poster/24458)
* [RLang: A Declarative Language for Describing Partial World Knowledge to Reinforcement Learning Agents](https://icml.cc/virtual/2023/poster/24583) <a href="#contents">⬆</a> 

## Interpretability

* [On the Impact of Knowledge Distillation for Model Interpretability](https://icml.cc/virtual/2023/poster/25147)
* [How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding](https://icml.cc/virtual/2023/poster/23534)
* [Mechanistic Mode Connectivity](https://icml.cc/virtual/2023/poster/24778) <a href="#contents">⬆</a> 

## Benchmarks

* [Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the Machiavelli Benchmark](https://icml.cc/virtual/2023/poster/23817)
* [Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://icml.cc/virtual/2023/poster/24093) <a href="#contents">⬆</a> 

## Other better understanding

* [Can Neural Network Memorization Be Localized?](https://icml.cc/virtual/2023/poster/23454)
* [Towards Understanding and Improving GFlowNet Training](https://icml.cc/virtual/2023/poster/24459)
* [What do CNNs Learn in the First Layer and Why? A Linear Systems Perspective](https://icml.cc/virtual/2023/poster/23991)
* [Are Neurons Actually Collapsed? On the Fine-Grained Structure in Neural Representations](https://icml.cc/virtual/2023/poster/23463) <a href="#contents">⬆</a> 

# Robustness 

## Adversarial Robustness

* [Preprocessors Matter! Realistic Decision-Based Attacks on Machine Learning Systems](https://icml.cc/virtual/2023/poster/24809)
* [Understanding Backdoor Attacks through the Adaptability Hypothesis](https://icml.cc/virtual/2023/poster/25104)
* [One-vs-the-Rest Loss to Focus on Important Samples in Adversarial Training](https://icml.cc/virtual/2023/poster/24734)
* [Adversarial Parameter Attack on Deep Neural Networks](https://icml.cc/virtual/2023/poster/24781)
* [A Critical Revisit of Adversarial Robustness in 3D Point Cloud Recognition with Diffusion-Driven Purification](https://icml.cc/virtual/2023/poster/24798)
* [On the Robustness of Randomized Ensembles to Adversarial Perturbations](https://icml.cc/virtual/2023/poster/24821)
* [Detecting Adversarial Data by Probing Multiple Perturbations Using Expected Perturbation Score](https://icml.cc/virtual/2023/poster/25017)
* [Improving Adversarial Robustness by Putting More Regularizations on Less Robust Samples](https://icml.cc/virtual/2023/poster/24095)
* [Detecting Adversarial Directions in Deep Reinforcement Learning to Make Robust Decisions](https://icml.cc/virtual/2023/poster/23576)
* [Identification of the Adversary from a Single Adversarial Example](https://icml.cc/virtual/2023/poster/23831)
* [Phase-aware Adversarial Defense for Improving Adversarial Robustness](https://icml.cc/virtual/2023/poster/24162)
* [How Many Perturbations Break This Model? Evaluating Robustness Beyond Adversarial Accuracy](https://icml.cc/virtual/2023/poster/24462)
* [Stratified Adversarial Robustness with Rejection](https://icml.cc/virtual/2023/poster/25097)
* [Eliminating Adversarial Noise via Information Discard and Robust Representation Restoration](https://icml.cc/virtual/2023/poster/25181)
* [NeRFool: Uncovering the Vulnerability of Generalizable Neural Radiance Fields against Adversarial Perturbations](https://icml.cc/virtual/2023/poster/23446)
* [Adversarially Robust PAC Learnability of Real-Valued Functions](https://icml.cc/virtual/2023/poster/23639)
* [Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples](https://icml.cc/virtual/2023/poster/23956)
* [Improving Adversarial Robustness of Deep Equilibrium Models with Explicit Regulations Along the Neural Dynamics](https://icml.cc/virtual/2023/poster/24207)
* [Probabilistic Categorical Adversarial Attack and Adversarial Training](https://icml.cc/virtual/2023/poster/24513)
* [Understanding the Impact of Adversarial Robustness on Accuracy Disparity](https://icml.cc/virtual/2023/poster/24915)
* [Adversarial Cheap Talk](https://icml.cc/virtual/2023/poster/24581)
* [Adversarial Learning of Distributional Reinforcement Learning](https://icml.cc/virtual/2023/poster/23895) <a href="#contents">⬆</a> 

## Out-of-Distribution

* [Hybrid Energy Based Model in the Feature Space for Out-of-Distribution Detection](https://icml.cc/virtual/2023/poster/24828)
* [Concept-based Explanations for Out-of-Distribution Detectors](https://icml.cc/virtual/2023/poster/25072)
* [Feed Two Birds with One Scone: Exploiting Wild Data for Both Out-of-Distribution Generalization and Detection](https://icml.cc/virtual/2023/poster/24661)
* [Unleashing Mask: Explore the Intrinsic Out-of-Distribution Detection Capability](https://icml.cc/virtual/2023/poster/25121)
* [Unsupervised Out-of-Distribution Detection with Diffusion Inpainting](https://icml.cc/virtual/2023/poster/23466)
* [Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization](https://icml.cc/virtual/2023/poster/25259) <a href="#contents">⬆</a> 

## Other Robustness

* [MultiRobustBench: Benchmarking Robustness Against Multiple Attacks](https://icml.cc/virtual/2023/poster/24654)
* [COLA: Orchestrating Error Coding and Learning for Robust Neural Network Inference Against Hardware Defects](https://icml.cc/virtual/2023/poster/24838)
* [BNN-DP: Robustness Certification of Bayesian Neural Networks via Dynamic Programming](https://icml.cc/virtual/2023/poster/23788)
* [Bidirectional Adaptation for Robust Semi-Supervised Learning with Inconsistent Data Distributions](https://icml.cc/virtual/2023/poster/24677)
* [On the Privacy-Robustness-Utility Trilemma in Distributed Learning](https://icml.cc/virtual/2023/poster/24614)
* [Computational Asymmetries in Robust Classification](https://icml.cc/virtual/2023/poster/23951)
* [Do Perceptually Aligned Gradients Imply Robustness?](https://icml.cc/virtual/2023/poster/24033)
* [Learning for Edge-Weighted Online Bipartite Matching with Robustness Guarantees](https://icml.cc/virtual/2023/poster/24251)
* [From Robustness to Privacy and Back](https://icml.cc/virtual/2023/poster/24344)
* [On the Robustness of Text Vectorizers](https://icml.cc/virtual/2023/poster/24672)
* [Byzantine-Robust Learning on Heterogeneous Data via Gradient Splitting](https://icml.cc/virtual/2023/poster/24965)
* [Robust Explanation for Free or At the Cost of Faithfulness](https://icml.cc/virtual/2023/poster/23896)
* [Disentangled Generative Models for Robust Prediction of System Dynamics](https://icml.cc/virtual/2023/poster/24499)
* [COLA: Orchestrating Error Coding and Learning for Robust Neural Network Inference Against Hardware Defects](https://icml.cc/virtual/2023/poster/24838)
* [Bidirectional Adaptation for Robust Semi-Supervised Learning with Inconsistent Data Distributions](https://icml.cc/virtual/2023/poster/24677) <a href="#contents">⬆</a> 

## Distributional Shift

* ["Why did the Model Fail?": Attributing Model Performance Changes to Distribution Shifts](https://icml.cc/virtual/2023/poster/24325)
* [Transcendental Idealism of Planner: Evaluating Perception from Planning Perspective for Autonomous Driving](https://icml.cc/virtual/2023/poster/25185)
* [ODS: Test-Time Adaptation in the Presence of Open-World Data Shift](https://icml.cc/virtual/2023/poster/24841)
* [Learning Control by Iterative Inversion](https://icml.cc/virtual/2023/poster/24748) <a href="#contents">⬆</a> 

# Fairness, Bias, Coordination 

## Fairness

* [Superhuman Fairness](https://icml.cc/virtual/2023/poster/23994)
* [Weak Proxies are Sufficient and Preferable for Fairness with Missing Sensitive Attributes](https://icml.cc/virtual/2023/poster/25141)
* [FAIRER: Fairness as Decision Rationale Alignment](https://icml.cc/virtual/2023/poster/25239)
* [Fairness in Matching under Uncertainty](https://icml.cc/virtual/2023/poster/24420)
* [Improving Fair Training under Correlation Shifts](https://icml.cc/virtual/2023/poster/24465)
* [Fair and Robust Estimation of Heterogeneous Treatment Effects for Policy Learning](https://icml.cc/virtual/2023/poster/25191)
* [Fair yet Asymptotically Equal Collaborative Learning](https://icml.cc/virtual/2023/poster/24844)
* [Fair and Optimal Classification via Post-Processing](https://icml.cc/virtual/2023/poster/25218)
* [Individually Fair Learning with One-Sided Feedback](https://icml.cc/virtual/2023/poster/23780)
* [Fair Neighbor Embedding](https://icml.cc/virtual/2023/poster/23798)
 <a href="#contents">⬆</a> 

## Bias

* [Towards Unbiased Training in Federated Open-world Semi-supervised Learning](https://icml.cc/virtual/2023/poster/25109)
* [Which Features are Learnt by Contrastive Learning? On the Role of Simplicity Bias in Class Collapse and Feature Suppression](https://icml.cc/virtual/2023/poster/23564)
* [Men Also Do Laundry: Multi-Attribute Bias Amplification](https://icml.cc/virtual/2023/poster/25124)

 <a href="#contents">⬆</a> 

## Coordination

* [Language Instructed Reinforcement Learning for Human-AI Coordination](https://icml.cc/virtual/2023/poster/24174)
* [Who Needs to Know? Minimal Knowledge for Optimal Coordination](https://icml.cc/virtual/2023/poster/23842)
* [Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for Test-Time Policy Adaptation](https://icml.cc/virtual/2023/poster/23624) <a href="#contents">⬆</a>


# LLMs & Scaling

## Coding

* [Repository-Level Prompt Generation for Large Language Models of Code](https://icml.cc/virtual/2023/poster/24506)
* [PAC Prediction Sets for Large Language Models of Code](https://icml.cc/virtual/2023/poster/24606)
* [LEVER: Learning to Verify Language-to-Code Generation with Execution](https://icml.cc/virtual/2023/poster/23632)
* [Fundamental Limits of Two-layer Autoencoders, and Achieving Them with Gradient Methods](https://icml.cc/virtual/2023/poster/25219)
* [LongCoder: A Long-Range Pre-trained Language Model for Code Completion](https://icml.cc/virtual/2023/poster/24793)
* [DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation](https://icml.cc/virtual/2023/poster/25042)
* [Coder Reviewer Reranking for Code Generation](https://icml.cc/virtual/2023/poster/24463)
* [Measuring the Impact of Programming Language Distribution](https://icml.cc/virtual/2023/poster/23720) <a href="#contents">⬆</a> 

## Scaling

* [Scaling Vision Transformers to 22 Billion Parameters](https://icml.cc/virtual/2023/poster/23644)
* [The case for 4-bit precision: k-bit Inference Scaling Laws](https://icml.cc/virtual/2023/poster/23915) <a href="#contents">⬆</a> 

## Optimisation

* [Understanding Int4 Quantization for Language Models: Latency Speedup,  Composability,  and Failure Cases](https://icml.cc/virtual/2023/poster/25069)
* [SLAMB: Accelerated Large Batch Training with Sparse Communication](https://icml.cc/virtual/2023/poster/23582)
* [Resurrecting Recurrent Neural Networks for Long Sequences](https://icml.cc/virtual/2023/poster/23579) <a href="#contents">⬆</a> 

## Instruction Fine-tuning

* [A Kernel-Based View of Language Model Fine-Tuning](https://icml.cc/virtual/2023/poster/24438)
* [POUF: Prompt-Oriented Unsupervised Fine-tuning for Large Pre-trained Models](https://icml.cc/virtual/2023/poster/24588) <a href="#contents">⬆</a> 

## Transformer

* [Emergent Agentic Transformer from Chain of Hindsight Experience](https://icml.cc/virtual/2023/poster/24228)
* [LookupFFN: Making Transformers Compute-lite for CPU inference](https://icml.cc/virtual/2023/poster/25048)
* [Predicting Ordinary Differential Equations with Transformers](https://icml.cc/virtual/2023/poster/23844)
* [Magneto: A Foundation Transformer](https://icml.cc/virtual/2023/poster/24404)
* [Brainformers: Trading Simplicity for Efficiency](https://icml.cc/virtual/2023/poster/24348) <a href="#contents">⬆</a> 

## General Attention

* [Infusing Lattice Symmetry Priors in Attention Mechanisms for Sample-Efficient Abstract Geometric Reasoning](https://icml.cc/virtual/2023/poster/23668)
* [Monotonic Location Attention for Length Generalization](https://icml.cc/virtual/2023/poster/23965)
* [CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling](https://icml.cc/virtual/2023/poster/24987)
* [On the Role of Attention in Prompt-tuning](https://icml.cc/virtual/2023/poster/23474) <a href="#contents">⬆</a> 

## General Language Models

* [Task-Specific Skill Localization in Fine-tuned Language Models](https://icml.cc/virtual/2023/poster/23540)
* [A Watermark for Large Language Models](https://icml.cc/virtual/2023/poster/24544)
* [The Wisdom of Hindsight Makes Language Models Better Instruction Followers](https://icml.cc/virtual/2023/poster/24009)
* [Large Language Models Can Be Easily Distracted by Irrelevant Context](https://icml.cc/virtual/2023/poster/24255)
* [Whose Opinions Do Language Models Reflect?](https://icml.cc/virtual/2023/poster/24316)
* [Why do Nearest Neighbor Language Models Work?](https://icml.cc/virtual/2023/poster/24893)
* [Exploring the Benefits of Training Expert Language Models over Instruction Tuning](https://icml.cc/virtual/2023/poster/23671)
* [SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models](https://icml.cc/virtual/2023/poster/25228)
* [Scaling Laws for Generative Mixed-Modal Language Models](https://icml.cc/virtual/2023/poster/24250)
* [Can Large Language Models Reason about Program Invariants?](https://icml.cc/virtual/2023/poster/23500)
* [Pretraining Language Models with Human Preferences](https://icml.cc/virtual/2023/poster/24005)
* [Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models](https://icml.cc/virtual/2023/poster/24048)
* [Hyena Hierarchy: Towards Larger Convolutional Language Models](https://icml.cc/virtual/2023/poster/24143)
* [Large Language Models Struggle to Learn Long-Tail Knowledge](https://icml.cc/virtual/2023/poster/25008)
* [Efficient Training of Language Models using Few-Shot Learning](https://icml.cc/virtual/2023/poster/23961)
* [SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot](https://icml.cc/virtual/2023/poster/24014)
* [LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation](https://icml.cc/virtual/2023/poster/23722)
* [Mu$^2$SLAM: Multitask, Multilingual Speech and Language Models](https://icml.cc/virtual/2023/poster/24244)
* [Reprogramming Pretrained Language Models for Antibody Sequence Infilling](https://icml.cc/virtual/2023/poster/23640)
* [Quantized Distributed Training of Large Models with Convergence Guarantees](https://icml.cc/virtual/2023/poster/24857)
* [SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient](https://icml.cc/virtual/2023/poster/24221)
* [PaLM-E: An Embodied Multimodal Language Model](https://icml.cc/virtual/2023/poster/23969)
* [Cramming: Training a Language Model on a single GPU in one day.](https://icml.cc/virtual/2023/poster/23926)
* [Tractable Control for Autoregressive Language Generation](https://icml.cc/virtual/2023/poster/23512)
* [Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time](https://icml.cc/virtual/2023/poster/23704) <a href="#contents">⬆</a> 

## Distillation

* [Revisiting Data-Free Knowledge Distillation with Poisoned Teachers](https://icml.cc/virtual/2023/poster/23792)
* [Less is More: Task-aware Layer-wise Distillation for Language Model Compression](https://icml.cc/virtual/2023/poster/24684) <a href="#contents">⬆</a> 

# Other Topics 

## Clustering 

* [Protecting Language Generation Models via Invisible Watermarking](https://icml.cc/virtual/2023/poster/24208)
* [XAI Beyond Classification: Interpretable Neural Clustering](https://icml.cc/virtual/2023/poster/25640)
* [Spurious Valleys and Clustering Behavior of Neural Networks](https://icml.cc/virtual/2023/poster/24648)
* [Generalized Reductions: Making any Hierarchical Clustering Fair and Balanced with Low Cost](https://icml.cc/virtual/2023/poster/23801) <a href="#contents">⬆</a> 

## Multi-agent

* [Multi-Agent Learning from Learners](https://icml.cc/virtual/2023/poster/23526)
* [Oracles & Followers: Stackelberg Equilibria in Deep Multi-Agent Reinforcement Learning](https://icml.cc/virtual/2023/poster/23525)
* [A Game-Theoretic Framework for Managing Risk in Multi-Agent Systems](https://icml.cc/virtual/2023/poster/23930)
 <a href="#contents">⬆</a> 

## Uncertainty

* [User-defined Event Sampling and Uncertainty Quantification in Diffusion Models for Physical Dynamical Systems](https://icml.cc/virtual/2023/poster/23781)
* [Uncertainty Estimation by Fisher Information-based Evidential Deep Learning](https://icml.cc/virtual/2023/poster/23912) <a href="#contents">⬆</a> 

## Actor-Critic

* [Warm-Start Actor-Critic: From Approximation Error to Sub-optimality Gap](https://icml.cc/virtual/2023/poster/23596)
* [Actor-Critic Alignment for Offline-to-Online Reinforcement Learning](https://icml.cc/virtual/2023/poster/23677) <a href="#contents">⬆</a> 

## Reliable, Intuitive and Explainable models
* [On the Relationship Between Explanation and Prediction: A Causal View](https://icml.cc/virtual/2023/poster/23810)
* [Learning Intuitive Policies Using Action Features](https://icml.cc/virtual/2023/poster/24494)
* [Great Models Think Alike: Improving Model Reliability via Inter-Model Latent Agreement](https://icml.cc/virtual/2023/poster/25195) <a href="#contents">⬆</a> 

## Open-endedness

* [Cooperative Open-ended Learning Framework for Zero-Shot Coordination](https://icml.cc/virtual/2023/poster/24863)
* [Human-Timescale Adaptation in an Open-Ended Task Space](https://icml.cc/virtual/2023/poster/24626) <a href="#contents">⬆</a> 

## Quantification, Detection, Dense

* [NNSplitter: An Active Defense Solution for DNN Model via Automated Weight Obfuscation](https://icml.cc/virtual/2023/poster/24038)
* [Topological Singularity Detection at Multiple Scales](https://icml.cc/virtual/2023/poster/24751)
* [Quantifying Human Priors over Social and Navigation Networks](https://icml.cc/virtual/2023/poster/24813)
* [Evaluating Self-Supervised Learning via Risk Decomposition](https://icml.cc/virtual/2023/poster/24022) <a href="#contents">⬆</a> 

## Machine Translation

* [Prompting Large Language Model for Machine Translation: A Case Study](https://icml.cc/virtual/2023/poster/25083)
* [Scaling Laws for Multilingual Neural Machine Translation](https://icml.cc/virtual/2023/poster/24419)
* [The Unreasonable Effectiveness of Few-shot Learning for Machine Translation](https://icml.cc/virtual/2023/poster/23572) <a href="#contents">⬆</a> 

## Multimodal

* [One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale](https://icml.cc/virtual/2023/poster/24135)
* [Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning](https://icml.cc/virtual/2023/poster/23575) <a href="#contents">⬆</a> 

## Generative Models

* [Denoising MCMC for Accelerating Diffusion-Based Generative Models](https://icml.cc/virtual/2023/poster/24745)
* [Muse: Text-To-Image Generation via Masked Generative Transformers](https://icml.cc/virtual/2023/poster/25125)
* [Raising the Cost of Malicious AI-Powered Image Editing](https://icml.cc/virtual/2023/poster/23740)
* [Temporally Consistent Transformers for Video Generation](https://icml.cc/virtual/2023/poster/24253)
* [SinFusion: Training Diffusion Models on a Single Image or Video](https://icml.cc/virtual/2023/poster/24630)
* [Efficient Sequence Transduction by Jointly Predicting Tokens and Durations](https://icml.cc/virtual/2023/poster/24805)
* [Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models](https://icml.cc/virtual/2023/poster/25060)
* [Invariant Slot Attention: Object Discovery with Slot-Centric Reference Frames](https://icml.cc/virtual/2023/poster/23824)
* [Global Context Vision Transformers](https://icml.cc/virtual/2023/poster/24768)
* [Neural Prediction Errors enable Analogical Visual Reasoning in Human Standard Intelligence Tests](https://icml.cc/virtual/2023/poster/24908) <a href="#contents">⬆</a> 

## Biology, Science

* [AbODE: Ab initio antibody design using conjoined ODEs](https://icml.cc/virtual/2023/poster/23763)
* [Structure-informed Language Models Are Protein Designers](https://icml.cc/virtual/2023/poster/24120)
* [QASA: Advanced Question Answering on Scientific Articles](https://icml.cc/virtual/2023/poster/24032) <a href="#contents">⬆</a> 
